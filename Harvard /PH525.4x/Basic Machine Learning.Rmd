---
title: "Conditional Expectation Exercises"
output: html_notebook
---

Conditional Expectations
Throughout this assessment it will be useful to remember that when our data are 0s and 1s, probabilities and expectations are the same thing. We can do the math, but here is an example in the form of R code:

```{r}
n = 1000
y = rbinom(n,1,0.25)
##proportion of ones Pr(Y)
sum(y==1)/length(y)
##expectaion of Y
mean(y)
```

Conditional Expectation Exercises #1
1 point possible (graded)
Generate some random data to imitate heights for men (0) and women (1):

```{r}
n = 10000
set.seed(1)
men = rnorm(n,176,7) #height in centimeters
women = rnorm(n,162,7) #height in centimeters
y = c(rep(0,n),rep(1,n))
x = round(c(men,women))
##mix it up
ind = sample(seq(along=y))
y = y[ind]
x = x[ind]
```

Treating the data generated above as the population, if we know someone is 176 cm tall, what it the probability that this person is a woman: Pr(洧녧=1|洧녦=176)=E(洧녧|洧녦=176)?

Answer code

```{r}
mean(y[x==176])

```

Conditional Expectation Exercises #2
1/1 point (graded)
Now make a plot of 洧냦(洧녧|洧녦=洧논) for x=seq(160,178) using the data generated in Conditional Expectation Exercises #1.

Suppose for each height 洧논 you predict 1 (female) if Pr(洧녧|洧녦=洧논)>0.5 and 0 (male) otherwise. What is the largest height for which you predict female ?


Answer code

```{r}
xs = seq(160,178)
pr =sapply(xs,function(x0) mean(y[x==x0]))
plot(xs,pr)
abline(h=0.5)
abline(v=168)
```

Smoothing Exercises


Make sure that you are using the correct random number generator (RNG) settings by calling the following command:

```{r}
RNGkind("Mersenne-Twister", "Inversion", "Rejection")

```
Smoothing Exercises #1
1 point possible (graded)
Use the data generated in a previous question about men's and women's heights:

```{r}
n = 10000
set.seed(1)
men = rnorm(n,176,7) #height in centimeters
women = rnorm(n,162,7) #height in centimeters
y = c(rep(0,n),rep(1,n))
x = round(c(men,women))
##mix it up
ind = sample(seq(along=y))
y = y[ind]
x = x[ind]
```
Set the seed at 5, set.seed(5), and take a random sample of 250 individuals from the population like this:

```{r}
set.seed(5)
N = 250
ind = sample(length(y),N)
Y = y[ind]
X = x[ind]
```

Use loess() to estimate 洧녭(洧논)=洧냦(洧녧|洧녦=洧논) using the default parameters. What is the predicted 洧녭(168)?

Answer code

```{r}
fit=loess(Y~X)
predict(fit,newdata=data.frame(X=168))

##Here is a plot
xs = seq(160,178)
Pr =sapply(xs,function(x0) mean(Y[X==x0]))
plot(xs,Pr)
fitted=predict(fit,newdata=data.frame(X=xs))
lines(xs,fitted)
```
Smoothing Exercises #2
1 point possible (graded)
The loess estimate above is a random variable thus we should compute its standard error. Use Monte Carlo simulation to compute the standard error of your estimate of 洧녭(168).

Set the seed to 5, set.seed(5), and perform 1000 simulations of the computations performed in question 2.7.1. Report the the SE of the loess based estimate.

```{r}
##plot plots are optional
set.seed(5)
B = 1000
N = 250
xs = seq(160,178)
plot(xs,xs,ylim=c(0,1),type="l")
res = replicate(B,{
  ind = sample(length(y),N)
  Y = y[ind]
  X = x[ind]
  fit=loess(Y~X)
  ##optional plots
  fitted=predict(fit,newdata=data.frame(X=xs))
  lines(xs,fitted)
  estimate = predict(fit,newdata=data.frame(X=168))
  return(estimate)
  })
library(rafalib)
popsd(res)

```
kNN and Cross Validation Exercises


Homework due May 9, 2021 07:01 +03
Changes in R since the creation of this material have altered the randomization code. You will need to include the following line in your code before you call set.seed(N) in order to obtain the correct answers:

```{r}
RNGkind(sample.kind = "Rounding")

```

Load the following dataset:

```{r}
library(GSE5859Subset)
data(GSE5859Subset)
```
And define the outcome and predictors. To make the problem more difficult, we will only consider autosomal genes:

```{r}
y = factor(sampleInfo$group)
X = t(geneExpression)
out = which(geneAnnotation$CHR%in%c("chrX","chrY"))
X = X[,-out]
```
Note, you will also need to load the following package:


```{r}
library(caret)

```
kNN and Cross Validation Exercises #1
1 point possible (graded)
Set the seed to 1, set.seed(1), then use the createFolds() function in the caret package to create 10 folds of y.

What is the 2nd entry in the fold 3?


Answer code 

```{r}
library(caret)
set.seed(1)
idx = createFolds(y, k=10)
idx[[3]][2]
sapply(idx,function(ind) table(y[ind])) ##make sure every fold has 0s and 1s

```
kNN and Cross Validation Exercises #2
1 point possible (graded)
For the following questions we are going to use kNN. We are going to consider a smaller set of predictors by filtering genes using t-tests. Specifically, we will perform a t-test and select the 洧녴 genes with the smallest p-values.

Let 洧녴=8 and 洧녲=5 and train kNN by leaving out the second fold, idx[[2]].

How many mistakes do we make on the test set? Remember it is indispensable that you perform the ttest on the training data.

Answer code 

```{r}
library(class)
library(genefilter)
m=8
k=5
ind = idx[[2]]
pvals = rowttests(t(X[-ind,]),factor(y[-ind]))$p.val
ind2 = order(pvals)[1:m]
predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
sum(predict!=y[ind])
```
kNN and Cross Validation Exercises #3
1 point possible (graded)
Now run the code for kNN and Cross Validation Exercises #2 for all 10 folds and keep track of the errors. What is our error rate (number of errors divided by number of predictions) ?


Answer code

```{r}
library(class)
library(genefilter)
m=8
k=5
result = sapply(idx,function(ind){
  pvals = rowttests(t(X[-ind,]),factor(y[-ind]))$p.val
  ind2 = order(pvals)[1:m]
  predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
  sum(predict!=y[ind])
})
sum(result)/length(y)
```
kNN and Cross Validation Exercises #4
0.0/2.0 points (graded)
Now we are going to select the best values of 洧녲 and 洧녴. Use the expand.grid() function to try out the following values:

```{r}
ms=2^c(1:11)
ks=seq(1,9,2)
params = expand.grid(k=ks,m=ms)
```

Now use sapply() or a for loop to obtain error rates for each of these pairs of parameters. Which pair of parameters minimizes the error rate?

k=

Answer code

```{r}
errors = apply(params,1,function(param){
  k =  param[1]
  m =  param[2]
  result = sapply(idx,function(ind){
    pvals = rowttests(t(X[-ind,]),factor(y[-ind]))$p.val
    ind2 = order(pvals)[1:m]
    predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
    sum(predict!=y[ind])
  })
  sum(result)/length(y)
  })
params[which.min(errors),]
##make a plot and confirm its just one min:
errors = matrix(errors,5,11)
library(rafalib)
mypar(1,1)
matplot(ms,t(errors),type="l",log="x")
legend("topright",as.character(ks),lty=seq_along(ks),col=seq_along(ks))
```
kNN and Cross Validation Exercises #5
1 point possible (graded)
Repeat question kNN and Cross Validation Exercises #4 but now perform the t-test filtering before the cross validation. Note how this biases the entire result and gives us much lower estimated error rates.

What is the minimum error rate?

Answer code

```{r}
pvals = rowttests(t(X),factor(y))$p.val
errors = apply(params,1,function(param){
  k =  param[1]
  m =  param[2]
  result = sapply(idx,function(ind){
    ind2 = order(pvals)[1:m]
    predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
    sum(predict!=y[ind])
  })
  sum(result)/length(y)
  })
min(errors)
##make a plot and compare to previous question
errors = matrix(errors,5,11)
library(rafalib)
mypar(1,1)
matplot(ms,t(errors),type="l",log="x")
legend("topright",as.character(ks),lty=seq_along(ks),col=seq_along(ks))
```

kNN and Cross Validation Exercises #6
1 point possible (graded)
Repeat the cross-validation we performed in question kNN and Cross Validation Exercises #4, but now instead of defining y as sampleInfo$group, use:

```{r}
y = factor(as.numeric(format( sampleInfo$date, "%m")=="06"))

```
What is the minimum error rate now?

Answer code

```{r}
errors = apply(params,1,function(param){
  k =  param[1]
  m =  param[2]
  result = sapply(idx,function(ind){
    pvals = rowttests(t(X[-ind,]),factor(y[-ind]))$p.val
    ind2 = order(pvals)[1:m]
    predict=knn(X[-ind,ind2],X[ind,ind2],y[-ind],k=k)
    sum(predict!=y[ind])
  })
  sum(result)/length(y)
  })
min(errors)
##make a plot and confirm its just one min:
errors = matrix(errors,5,11)
library(rafalib)
mypar(1,1)
matplot(ms,t(errors),type="l",log="x")
legend("topright",as.character(ks),lty=seq_along(ks),col=seq_along(ks))
```

